1. TensorRT warning
https://discuss.tensorflow.org/t/unable-to-get-tensorflow-working-correctly/18981

2. Tensoflow Installation Guide
https://www.tensorflow.org/install/pip#step-by-step_instructions

3. Tensorflow GPU 확인 방법
https://precommer.tistory.com/78

4. AWS SageMaker on GPU
https://stackoverflow.com/questions/60868257/aws-sagemaker-on-gpu

>>>
5. Ask with ChatGPT
 To configure Amazon SageMaker Studio notebook to use TensorFlow with the CUDA library, you can follow these steps:

1. Launch SageMaker Studio: First, open the Amazon SageMaker console and navigate to Amazon SageMaker Studio. Access the SageMaker Studio control panel.

2. a New Notebook: Inside SageMaker Studio, create a new notebook or open an one that you want to configure to use TensorFlow with the NVIDIA library.

3. Choose TensorFlow Kernel: When creating a new notebook, select a TensorFlow kernel, which comes pre-configured with NVIDIA CUDA dependencies. If using a pre-existing notebook, you can change the kernel to a TensorFlow compatible one by clicking on the "Change Kernel" option in the notebook interface and selecting a TensorFlow kernel.

4. Install additional packages (if needed): Depending on your specific requirements, you may need to install additional TensorFlow or CUDA versions. You can use the notebook's terminal or a cell in the notebook to run installation commands, such as pip tensorflow-gpu to get the GPU-supported version of TensorFlow.

5. Configure for GPU acceleration: To ensure that TensorFlow is configured to use the NVIDIA CUDA library for GPU acceleration, you can add configuration code to your. For example, you can use TensorFlow's GPU device detection code to verify that TensorFlow is detecting and utilizing the GPU:

```python
import tensorflow as tf
print("Num GPUs Available: ", len(tf.experimental.list_physical_devices('GPU')))
```

6. Verify GPU usage: Run a test to verify that TensorFlow is indeed using the GPU for computation. You can do this by running a TensorFlow-based computation and checking the GPU usage using command-line tools available in the notebook environment.

By these steps, you can configure your Amazon SageMaker Studio notebook to use TensorFlow with NVIDIA CUDA library for GPU acceleration.  
<<<

6. Python 패키지를 Amazon SageMaker 노트북 인스턴스의 Conda 환경에 설치하려면 어떻게 해야 하나요?
https://repost.aws/ko/knowledge-center/sagemaker-python-package-conda

7. TF-TRT Warning: Could not find TensorRT
https://github.com/tensorflow/tensorflow/issues/61468

>>>
8. Check if TensorFlow is correctly configured in fresh Amazon Linux DLAMI.
텐서플로우가 아마존 리눅스 DLAMI 인스턴스에 정상적으로 설정되었는지 확인하는 방법
 To check if TensorFlow framework is correctly installed on Amazon Linux, you can use the following steps:

0. LD_LIBRARY_PATH 확인
echo $LD_LIBRARY_PATH 
ls -al /usr/local/
ls -al /etc/alternatives/cuda
ls -al /usr/local/cuda-11.8
python --version

1. Open a terminal window on your Amazon Linux instance.

2. Activate the virtual environment (if TensorFlow was installed within a virtual environment):
   ```bash
   source <path_to_virtual_env>/bin/activate
   ```

3. Launch a Python interpreter:
   ```bash
   python
   ```

4. Import TensorFlow within the Python interpreter:
   ```python
   import tensorflow as tf
   ```

5. If TensorFlow is imported without any error messages, it indicates that TensorFlow is correctly installed and accessible within your Python environment.

6. You can also verify the TensorFlow version to ensure it matches the expected version:
   ```python
   print(tf.__version__)
   ```
7. Another command
print(tf.test.is_built_with_cuda())

from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))



tf.test.gpu_device_name()

Executing these steps will confirm if TensorFlow is properly installed and accessible within your Amazon Linux environment.  
<<<

>>>
9. Conda 설정
To install and configure a Python conda environment with TensorFlow GPU, follow these steps:

1. **Install Miniconda**:
   - If you don't have Miniconda or Anaconda installed, download and install Miniconda from the official website: https://docs.conda.io/en/latest/miniconda.html

2. **Create a New Conda Environment**:
   - Open a terminal or command prompt.
   - Create a new conda environment using the following command:
     ```
     conda create -n myenv python=3.8
     ```
   - Replace "myenv" with the name you want for your environment. This command creates a new environment with Python 3.8 installed.

3. **Activate the New Environment**:
   - Activate the new environment using the following command:
     - On Windows:
       ```
       conda activate myenv
       ```
     - On macOS and Linux:
       ```
       source activate myenv
       ```

4. **Install TensorFlow-GPU**:
   - Once the environment is activated, install TensorFlow-GPU using pip:
     ```
     pip install tensorflow-gpu
     ```

5. **Install CUDA Toolkit and cuDNN**:
   - Download and install the CUDA Toolkit and cuDNN from the NVIDIA website. Make sure to install versions compatible with your GPU and TensorFlow version. Follow the installation instructions provided by NVIDIA.

6. **Test the Installation**:
   - Create a Python script and import TensorFlow to test if the GPU is being utilized:
     ```python
     import tensorflow as tf
     print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
     ```

7. **Deactivate the Environment** (Optional):
   - When you're done working in the environment, deactivate it using the following command:
     ```
     conda deactivate
     ```

These steps should help you install and configure a Python conda environment with TensorFlow-GPU. Remember to refer to the official documentation and installation guides for the specific versions and system requirements.  
<<<

10. pip install tensorflow[and-cuda]로 설치할 경우에는 아래와 같이 CUDA Driver 버전이 안 맞는 에러 발생
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/sagemaker-user/generative-ai-fundamentals/tf_env/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py", line 172, in gpu_device_name
    for x in device_lib.list_local_devices():
  File "/home/sagemaker-user/generative-ai-fundamentals/tf_env/lib/python3.10/site-packages/tensorflow/python/client/device_lib.py", line 41, in list_local_devices
    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)
RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version

여기에 보면
https://blog.tensorflow.org/2023/11/whats-new-in-tensorflow-2-15.html

The tensorflow pip package has a new, optional installation method for Linux that installs necessary NVIDIA CUDA libraries through pip.
As long as the NVIDIA driver is already installed on the system, you may now run pip install tensorflow[and-cuda] to install TensorFlow's NVIDIA CUDA library dependencies in the Python environment.
Aside from the NVIDIA driver, no other pre-existing NVIDIA CUDA packages are necessary. In TensorFlow 2.15, CUDA has been upgraded to version 12.2.

11. Driver incompatibility for triton image on sagemaker instance #3813
https://github.com/triton-inference-server/server/issues/3813

12. CUDA 드라이버 업그레이드
https://arinzeakutekwe.medium.com/how-to-configure-nvidia-gpu-to-work-with-tensorflow-2-on-aws-sagemaker-1be98b9db464

* 으~ 정말~ SageMaker Studio는 Root 액세스를 허용하지 않아서 드라이버 업그레이드 불가능
https://github.com/aws/studio-lab-examples/issues/118
https://docs.securestate.vmware.com/rule-docs/aws-sagemaker-notebookinstance-root-access-enabled